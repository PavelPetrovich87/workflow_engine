# Feature Specification: LLM Generation Node

**Feature Branch**: `001-llm-generation`
**Created**: 2026-01-08
**Status**: Draft
**Input**: User description: "We need to create this new LLM node strategy: 1. ðŸ§  LLM Generation Node (llm-generation) The core workhorse. * Input: prompt (string), model (e.g., 'gpt-4'), temperature. * Output: result (text). * Use Case: Writing emails, summarizing content, generating code."

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Basic Text Generation (Priority: P1)

As a workflow creator, I want to use an LLM node to generate text based on a static prompt so that I can automate creative tasks like writing emails.

**Why this priority**: Core functionality of the feature.

**Independent Test**: Create a workflow with a single `llm-generation` node, set a static prompt "Hello world", run it, and verify output.

**Acceptance Scenarios**:

1. **Given** a new workflow, **When** I add an `llm-generation` node and set the prompt to "Write a haiku", **Then** the execution result should contain a valid haiku.
2. **Given** the node configuration, **When** I set the temperature to 0.0, **Then** the output should be deterministic (or highly consistent) across multiple runs.

---

### User Story 2 - Dynamic Prompting (Priority: P1)

As a workflow creator, I want to pass data from previous nodes into the LLM prompt so that I can summarize content or process dynamic inputs.

**Why this priority**: Essential for chaining nodes and real-world workflows.

**Independent Test**: Create a workflow where Node A outputs text, and Node B (LLM) uses Node A's output in its prompt.

**Acceptance Scenarios**:

1. **Given** a workflow with an upstream trigger providing text, **When** I map that text to the `llm-generation` prompt input, **Then** the LLM should generate a response based on that specific input text.

---

### Edge Cases

- **Empty Prompt**: What happens if the prompt is empty string? (Should probably error or return empty).
- **Invalid Model**: What happens if the configured model is not supported? (Should throw `PROVIDER_ERROR`).
- **Context Limit**: What happens if the input is too long for the model context? (Should throw `PROVIDER_ERROR` or truncate, standard LLM behavior).
- **API Failure**: What happens if the API is down? (Should throw `PROVIDER_ERROR` caught by BaseLlmStrategy).

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST provide a new node strategy type `llm-generation` in the registry.
- **FR-002**: The node MUST accept `prompt` (string) as an input property.
- **FR-003**: The node MUST accept `model` (string) as a configuration option (default to 'gpt-4' or 'gemini-pro' depending on provider).
- **FR-004**: The node MUST accept `temperature` (number) as a configuration option, defaulting to a reasonable value (e.g., 0.7).
- **FR-005**: The node MUST output a `result` field containing the generated text string.
- **FR-006**: The execution strategy MUST utilize the system's standard mechanism for API key management and error handling to ensure consistency with other AI nodes.
- **FR-007**: The node MUST validate that `prompt` is not empty before execution.

### Key Entities

- **Generation Configuration**: Settings for the generation process, including model selection, temperature control, and optional system instructions.
- **Generation Input**: The dynamic text prompt provided to the node.
- **Generation Output**: The resulting text generated by the model.

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Users can successfully execute a workflow containing the `llm-generation` node and receive a non-empty string output.
- **SC-002**: Configuration changes (model, temperature) are correctly reflected in the API call (verifiable via logs or mocks).
- **SC-003**: System accurately identifies and reports failures due to missing credentials or invalid configurations, distinguishing them from generic errors.
